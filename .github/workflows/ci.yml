name: Evals

on:
  pull_request:
    types:
      - opened
      - synchronize
      - labeled

concurrency:
  group: ${{ github.ref }}
  cancel-in-progress: true

env:
  EVAL_MODELS: "gpt-4o,gpt-4o-mini,claude-3-5-sonnet-latest"
  EVAL_CATEGORIES: "observe,act,combination,extract,text_extract"

jobs:
  determine-evals:
    runs-on: ubuntu-latest
    outputs:
      run-extract: ${{ steps.check-labels.outputs.run-extract }}
      run-act: ${{ steps.check-labels.outputs.run-act }}
      run-observe: ${{ steps.check-labels.outputs.run-observe }}
      run-text-extract: ${{ steps.check-labels.outputs.run-text-extract }}
    steps:
      - id: check-labels
        run: |
          # Default to running all tests on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "run-extract=true" >> $GITHUB_OUTPUT
            echo "run-act=true" >> $GITHUB_OUTPUT
            echo "run-observe=true" >> $GITHUB_OUTPUT
            echo "run-text-extract=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Check for specific labels
          echo "run-extract=${{ contains(github.event.pull_request.labels.*.name, 'extract') }}" >> $GITHUB_OUTPUT
          echo "run-act=${{ contains(github.event.pull_request.labels.*.name, 'act') }}" >> $GITHUB_OUTPUT
          echo "run-observe=${{ contains(github.event.pull_request.labels.*.name, 'observe') }}" >> $GITHUB_OUTPUT
          echo "run-text-extract=${{ contains(github.event.pull_request.labels.*.name, 'text-extract') }}" >> $GITHUB_OUTPUT

  run-lint:
    needs: [determine-evals]
    runs-on: ubuntu-latest
    steps:
      - name: Dummy Lint
        run: echo "Pretend lint completed successfully"

  run-build:
    needs: [run-lint]
    runs-on: ubuntu-latest
    steps:
      - name: Dummy Build
        run: echo "Pretend build completed successfully"

  run-e2e-tests:
    needs: [run-lint, run-build]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Dummy E2E Tests
        run: echo "Pretend E2E tests succeeded"

  run-e2e-bb-tests:
    needs: [run-e2e-tests]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    if: >
      github.event_name == 'push' ||
      (github.event_name == 'pull_request' && github.event.pull_request.head.repo.full_name == github.repository)
    steps:
      - name: Dummy E2E BB Tests
        run: echo "Pretend E2E BB tests succeeded"

  run-combination-evals:
    needs: [run-e2e-bb-tests, determine-evals]
    runs-on: ubuntu-latest
    timeout-minutes: 5
    steps:
      - name: Dummy Combination Eval
        run: |
          echo "Pretending to run combination eval..."
          # Create a dummy eval-summary.json with a PASSING combo score
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyCombinationTest",
            "categories": {
              "combination": 85
            }
          }
          EOF

      - name: Log Combination Evals Performance
        run: |
          if [ ! -f eval-summary.json ]; then
            echo "Eval summary not found for combination category. Failing CI."
            exit 1
          fi

          experimentName=$(jq -r '.experimentName' eval-summary.json)
          combination_score=$(jq '.categories.combination' eval-summary.json)

          echo "Combination category score: ${combination_score}%"
          echo "View results at https://example.com/experiments/${experimentName}"

  run-act-evals:
    needs: [run-combination-evals]
    if: needs.determine-evals.outputs.run-act == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    concurrency:
      group: labeled-evals-${{ github.run_id }}
      cancel-in-progress: false
    steps:
      - name: Dummy Act Evals
        run: |
          echo "Pretending to run ACT eval..."
          # Create a passing ACT score
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyActExp",
            "categories": {
              "act": 82
            }
          }
          EOF

      - name: Log Act Evals Performance
        run: |
          if [ ! -f eval-summary.json ]; then
            echo "Eval summary not found for act category. Failing CI."
            exit 1
          fi
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          act_score=$(jq '.categories.act' eval-summary.json)
          echo "Act category score: $act_score%"
          if (( $(echo "$act_score < 80" | bc -l) )); then
            echo "Act category score <80%. Failing CI."
            exit 1
          fi

  run-extract-evals:
    needs: [run-combination-evals]
    if: needs.determine-evals.outputs.run-extract == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    concurrency:
      group: labeled-evals-${{ github.run_id }}
      cancel-in-progress: false
    steps:
      - name: Step 1 - Dummy Extract (domExtract)
        run: |
          echo "Pretending to run domExtract..."
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyExtractDomExp",
            "categories": {
              "extract": 85
            }
          }
          EOF

      - name: Save Extract Dom Results
        run: mv eval-summary.json eval-summary-extract-dom.json

      - name: Step 2 - Dummy Extract (textExtract)
        run: |
          echo "Pretending to run textExtract..."
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyExtractTextExp",
            "categories": {
              "extract": 90
            }
          }
          EOF

      - name: Save Extract Text Results
        run: mv eval-summary.json eval-summary-extract-text.json

      - name: Step 3 - Compare Extract Evals
        run: |
          dom_score=$(jq '.categories.extract' eval-summary-extract-dom.json)
          text_score=$(jq '.categories.extract' eval-summary-extract-text.json)
          echo "DomExtract score: $dom_score%"
          echo "TextExtract score: $text_score%"
          if (( $(echo "$dom_score < 80" | bc -l) )); then
            echo "DomExtract <80%. Failing CI."
            exit 1
          fi

  run-text-extract-evals:
    needs: [run-combination-evals]
    if: needs.determine-evals.outputs.run-text-extract == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    concurrency:
      group: labeled-evals-${{ github.run_id }}
      cancel-in-progress: false
    steps:
      - name: Step 1 - Dummy text_extract (textExtract)
        run: |
          echo "Pretending to run text_extract with textExtract..."
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyTextExtractTextExp",
            "categories": {
              "text_extract": 88
            }
          }
          EOF

      - name: Save text_extract Text Results
        run: mv eval-summary.json eval-summary-text_extract-text.json

      - name: Step 2 - Dummy text_extract (domExtract)
        run: |
          echo "Pretending to run text_extract with domExtract..."
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyTextExtractDomExp",
            "categories": {
              "text_extract": 93
            }
          }
          EOF

      - name: Save text_extract Dom Results
        run: mv eval-summary.json eval-summary-text_extract-dom.json

      - name: Step 3 - Compare text_extract Evals
        run: |
          text_score=$(jq '.categories.text_extract' eval-summary-text_extract-text.json)
          dom_score=$(jq '.categories.text_extract' eval-summary-text_extract-dom.json)
          echo "TextExtract text_extract score: $text_score%"
          echo "DomExtract text_extract score: $dom_score%"
          if (( $(echo "$text_score < 80" | bc -l) )); then
            echo "textExtract text_extract <80%. Failing CI."
            exit 1
          fi

  run-observe-evals:
    needs: [run-combination-evals]
    if: needs.determine-evals.outputs.run-observe == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    concurrency:
      group: labeled-evals-${{ github.run_id }}
      cancel-in-progress: false
    steps:
      - name: Dummy Observe Evals
        run: |
          echo "Pretending to run OBSERVE eval..."
          # You can simulate a failing score (e.g. 75) or passing (85)
          observe_score=85
          cat <<EOF > eval-summary.json
          {
            "experimentName": "dummyObserveExp",
            "categories": {
              "observe": $observe_score
            }
          }
          EOF

      - name: Log Observe Evals Performance
        run: |
          if [ ! -f eval-summary.json ]; then
            echo "Eval summary not found for observe category. Failing CI."
            exit 1
          fi
          experimentName=$(jq -r '.experimentName' eval-summary.json)
          observe_score=$(jq '.categories.observe' eval-summary.json)
          echo "Observe category score: $observe_score%"
          if (( $(echo "$observe_score < 80" | bc -l) )); then
            echo "Observe category score <80%. Failing CI."
            exit 1
          fi
